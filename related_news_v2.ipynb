{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'module')))\n",
    "from openTable import *\n",
    "from AccesDB import *\n",
    "from preprocessing import preprocessing_text as pre\n",
    "\n",
    "# import gensim\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim import similarities\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# import spacy\n",
    "from spacy.lang.id import Indonesian,stop_words\n",
    "nlp = Indonesian()  # use directly\n",
    "stopwords = stop_words.STOP_WORDS \n",
    "stopwords |= {\"nya\",\"jurusan\",\"jurus\",\"the\",\"of\"}\n",
    "\n",
    "from json import loads\n",
    "from ast import literal_eval\n",
    "from pickle import load\n",
    "from re import sub\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datetime import date,timedelta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text = pre.remove_tag(text) #Remove Tag\n",
    "    text = pre.lower(text) #Lower\n",
    "    text = pre.remove_link(text) #Remove Link\n",
    "    text = pre.alphabet_only(text) #Get Alphabet\n",
    "    text = sub(r'sobat pintar','',text) # sorry:(\n",
    "    text = pre.remove_whitespace(text) #Remove Whitespace\n",
    "    text = [token.text for token in nlp(text)] #Token\n",
    "    text = pre.slang(text)\n",
    "    text = [token.lemma_ for token in nlp(text) if token.lemma_ not in stopwords] #Lemma & stopword\n",
    "    \n",
    "    return text\n",
    "\n",
    "def load_model():\n",
    "    #Load Model\n",
    "    loaded_model = LdaModel.load('../data/lda.h5')\n",
    "    \n",
    "    #Load Corpus\n",
    "    file = open('../data/corpus_LDA.pkl','rb')\n",
    "    loaded_corpus = load(file)\n",
    "    \n",
    "    #Load Dictionary\n",
    "    file = open('../data/dictionary_LDA.gensim','rb')\n",
    "    loaded_dict = load(file)\n",
    "    \n",
    "    #Load TFIDF\n",
    "    file = open('../data/tfidf.h5','rb')\n",
    "    loaded_tfidf = load(file)\n",
    "    \n",
    "    #Load encoder\n",
    "    dict_encoder = loads(open(\"../data/dict_encoder.txt\", 'r').read())\n",
    "    \n",
    "    return loaded_model,loaded_corpus,loaded_dict,loaded_tfidf,dict_encoder\n",
    "\n",
    "def encode(docId,dict_encoder):\n",
    "    entryId = []\n",
    "    \n",
    "    for i in docId:\n",
    "        result = dict_encoder.get(str(i))\n",
    "        entryId.append(result)\n",
    "        \n",
    "    return entryId    \n",
    "\n",
    "def new_user(koneksi,event):\n",
    "    result = []\n",
    "    entryId = event['entryId']\n",
    "    try:\n",
    "        loaded_model,loaded_corpus,loaded_dict,loaded_tfidf,dict_encoder = load_model()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return result\n",
    "#         pass\n",
    "    try:\n",
    "        #Get Doc\n",
    "        statement = \" WHERE entryId = {}\"\n",
    "        data,status = open_table(koneksi,['entryId','content'],'BlogsEntry',statement=statement.format(entryId))\n",
    "        text = data[1].values[0]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return result\n",
    "#         pass\n",
    "    try:\n",
    "        #Test new document\n",
    "        text = preprocessing(text) #Preprocessing\n",
    "        # Add bigrams and trigrams to docs,minimum count 10 means only that appear 10 times or more.\n",
    "        bigram = Phrases([text], min_count=3)\n",
    "        for token in bigram[text]:\n",
    "            if '_' in token:\n",
    "                # Token is a bigram, add to document.\n",
    "                text.append(token)\n",
    "\n",
    "        bow = loaded_dict.doc2bow(text)\n",
    "\n",
    "        vector = loaded_tfidf[bow]  # apply model to the first corpus document\n",
    "        vector = sorted(vector, key=lambda tup: tup[1],reverse=True) #Sort\n",
    "        keyword = [loaded_dict[x[0]] for x in vector]\n",
    "\n",
    "        new_bow = loaded_dict.doc2bow(keyword)\n",
    "    \n",
    "        lda_index = similarities.MatrixSimilarity(loaded_model[loaded_corpus])\n",
    "    \n",
    "        query = lda_index[loaded_model[new_bow]]\n",
    "        # # Sort the similarities\n",
    "        sort_sim = sorted(enumerate(query), key=lambda item: -item[1])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return result\n",
    "#         pass\n",
    "    \n",
    "    try:\n",
    "        result = [x[0] for x in sort_sim] #Get Univ ID\n",
    "        result = encode(result,dict_encoder)\n",
    "        result = [x for x in result if x !=entryId] #Remove Input EntryId\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "#         pass\n",
    "    \n",
    "    return result[:10]\n",
    "\n",
    "def save_recommendation(koneksi,entryId,recommendation,last_update):    \n",
    "    table = \"related_news_lda\"\n",
    "    column = ['entryId','recommendation','tanggal']\n",
    "    value = [entryId,str(recommendation),last_update]\n",
    "    status = to_db(koneksi,table,column,value)\n",
    "\n",
    "def get_similar_article(event):    \n",
    "    related = []\n",
    "    entryId = event['entryId']\n",
    "    \n",
    "    try:\n",
    "        #DS\n",
    "        ds_server,ds_koneksi = Connection_2()\n",
    "        print('open connection 2')\n",
    "\n",
    "        today = date.today()\n",
    "        refreshtime = today - timedelta(days=4)\n",
    "        statement = ' where entryId = {}'\n",
    "        recommendation,status = open_table_ds(ds_koneksi,['*'],'related_news_lda',statement=statement.format(entryId))\n",
    "        print('get data')\n",
    "        \n",
    "        ds_koneksi.close()\n",
    "        ds_server.stop()\n",
    "        print('close connection 2')\n",
    "    except:\n",
    "        ds_koneksi.close()\n",
    "        ds_server.stop()\n",
    "        print('close connection 2')\n",
    "        return related\n",
    "\n",
    "    #First Time\n",
    "    if not recommendation:\n",
    "        print('first time')\n",
    "        #Prod\n",
    "        try:\n",
    "            prod_server,prod_koneksi = Connection()\n",
    "            print('open connection')\n",
    "            result = new_user(prod_koneksi,event)\n",
    "            print('calculating')\n",
    "            prod_koneksi.close()\n",
    "            prod_server.stop()\n",
    "            print('close connection')\n",
    "        except:\n",
    "            prod_koneksi.close()\n",
    "            prod_server.stop()\n",
    "            print('close connection')\n",
    "            return related\n",
    "        \n",
    "        try:\n",
    "            ds_server,ds_koneksi = Connection_2()\n",
    "            print('open connection 2')\n",
    "            save_recommendation(ds_koneksi,entryId,result,today)\n",
    "            ds_koneksi.commit()\n",
    "            print('save')\n",
    "            \n",
    "            ds_koneksi.close()\n",
    "            ds_server.stop()\n",
    "            print('close connection 2')\n",
    "        except:           \n",
    "            ds_koneksi.close()\n",
    "            ds_server.stop()\n",
    "            print('close connection 2')\n",
    "            \n",
    "            return related\n",
    "\n",
    "    else:\n",
    "        recommendation = recommendation[0]\n",
    "        recommendation_refreshtime = recommendation[2]\n",
    "        print('get refresh time')\n",
    "        #Refresh Time\n",
    "        if refreshtime > recommendation_refreshtime:\n",
    "            #Prod\n",
    "            try:\n",
    "                prod_server,prod_koneksi = Connection()\n",
    "                print('open connection')\n",
    "                result = new_user(prod_koneksi,event)\n",
    "                print('calculating')\n",
    "                prod_koneksi.close()\n",
    "                prod_server.stop()\n",
    "                print('close connection')\n",
    "            except:\n",
    "                prod_koneksi.close()\n",
    "                prod_server.stop()\n",
    "                print('close connection')\n",
    "                \n",
    "                return related\n",
    "            \n",
    "            statement = ' where entryId = {}'\n",
    "            data = {\n",
    "                'recommendation':result,\n",
    "                'tanggal':today\n",
    "            }\n",
    "            try:\n",
    "                ds_server,ds_koneksi = Connection_2()\n",
    "                print('open connection 2')\n",
    "                status = update_db(ds_koneksi,'related_news_lda',data,statement=statement.format(entryId))\n",
    "                ds_koneksi.commit()\n",
    "                print('save')\n",
    "                \n",
    "                ds_koneksi.close()\n",
    "                ds_server.stop()\n",
    "                print('close connection 2')\n",
    "            except:\n",
    "                ds_koneksi.close()\n",
    "                ds_server.stop()\n",
    "                print('close connection 2')\n",
    "                \n",
    "                return related\n",
    "        #Already Exist\n",
    "        else:\n",
    "            result = literal_eval(recommendation[1])\n",
    "            print('already exsist')\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import process_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open connection 2\n",
      "get data\n",
      "close connection 2\n",
      "get refresh time\n",
      "open connection\n",
      "calculating\n",
      "close connection\n",
      "open connection 2\n",
      "save\n",
      "close connection 2\n",
      "[5613511, 31895184, 451726, 13133339, 17504371, 1961192, 54707939, 13191900, 576210, 50549127]\n",
      "4.265625\n"
     ]
    }
   ],
   "source": [
    "t = process_time()\n",
    "#do some stuff\n",
    "#Prod\n",
    "# prod_server,prod_koneksi = Connection()\n",
    "\n",
    "event = {\"entryId\":88003958}\n",
    "print(get_similar_article(event))\n",
    "\n",
    "# prod_koneksi.close()\n",
    "# prod_server.stop()\n",
    "elapsed_time = process_time() - t\n",
    "print(elapsed_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
