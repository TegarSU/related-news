{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'module')))\n",
    "from openTable import *\n",
    "from preprocessing import preprocessing_text as pre\n",
    "\n",
    "# import gensim\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim import similarities\n",
    "\n",
    "# import spacy\n",
    "from spacy.lang.id import Indonesian,stop_words\n",
    "nlp = Indonesian()  # use directly\n",
    "stopwords = stop_words.STOP_WORDS \n",
    "stopwords |= {\"nya\",\"jurusan\",\"jurus\",\"the\",\"of\"}\n",
    "\n",
    "from json import loads\n",
    "from ast import literal_eval\n",
    "from pickle import load\n",
    "from re import sub\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = pre.remove_tag(text) #Remove Tag\n",
    "    text = pre.lower(text) #Lower\n",
    "    text = pre.remove_link(text) #Remove Link\n",
    "    text = pre.alphabet_only(text) #Get Alphabet\n",
    "    text = sub(r'sobat pintar','',text) # sorry:(\n",
    "    text = pre.remove_whitespace(text) #Remove Whitespace\n",
    "    text = [token.text for token in nlp(text)] #Token\n",
    "    text = pre.slang(text)\n",
    "    text = [token.lemma_ for token in nlp(text) if token.lemma_ not in stopwords] #Lemma & stopword\n",
    "    \n",
    "    return text\n",
    "\n",
    "def load_model():\n",
    "    #Load Model\n",
    "    loaded_model = LdaModel.load('../data/lda.h5')\n",
    "    \n",
    "    #Load Corpus\n",
    "    file = open('../data/corpus_LDA.pkl','rb')\n",
    "    loaded_corpus = load(file)\n",
    "    \n",
    "    #Load Dictionary\n",
    "    file = open('../data/dictionary_LDA.gensim','rb')\n",
    "    loaded_dict = load(file)\n",
    "    \n",
    "    #Load encoder\n",
    "    dict_encoder = loads(open(\"../data/dict_encoder.txt\", 'r').read())\n",
    "    \n",
    "    return loaded_model,loaded_corpus,loaded_dict,dict_encoder\n",
    "\n",
    "def encode(docId,dict_encoder):\n",
    "    entryId = []\n",
    "    \n",
    "    for i in docId:\n",
    "        result = dict_encoder.get(str(i))\n",
    "        entryId.append(result)\n",
    "        \n",
    "    return entryId    \n",
    "\n",
    "def get_similar(event):\n",
    "    try:\n",
    "        entryId = event['entryId']\n",
    "        loaded_model,loaded_corpus,loaded_dict,dict_encoder = load_model()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "#         pass\n",
    "    \n",
    "    try:\n",
    "        #Get Doc\n",
    "        statement = \" WHERE entryId = {}\"\n",
    "        data = open_table(['entryId','content'],'BlogsEntry',statement=statement.format(entryId))\n",
    "        text = data[1].values[0]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "#         pass\n",
    "    \n",
    "    try:\n",
    "        #Test new document\n",
    "        bow = loaded_dict.doc2bow(preprocessing(text))\n",
    "    \n",
    "        lda_index = similarities.MatrixSimilarity(loaded_model[loaded_corpus])\n",
    "    \n",
    "        query = lda_index[loaded_model[bow]]\n",
    "        # # Sort the similarities\n",
    "        sort_sim = sorted(enumerate(query), key=lambda item: -item[1])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "#         pass\n",
    "    \n",
    "    try:\n",
    "        result = [x[0] for x in sort_sim] #Get Univ ID\n",
    "        result = encode(result,dict_encoder)\n",
    "        result.remove(entryId) #Remove Input EntryId\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "#         pass\n",
    "    \n",
    "    return result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import process_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46194481, 52399708, 46302531, 58216163, 20296423]\n",
      "1.796875\n"
     ]
    }
   ],
   "source": [
    "t = process_time()\n",
    "#do some stuff\n",
    "event = {'entryId':63282109}\n",
    "print(get_similar(event))\n",
    "elapsed_time = process_time() - t\n",
    "print(elapsed_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
