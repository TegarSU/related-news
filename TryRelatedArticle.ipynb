{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'module')))\n",
    "from openTable import *\n",
    "from preprocessing import preprocessing_text as pre\n",
    "\n",
    "# import gensim\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim import similarities\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# import spacy\n",
    "from spacy.lang.id import Indonesian,stop_words\n",
    "nlp = Indonesian()  # use directly\n",
    "stopwords = stop_words.STOP_WORDS \n",
    "stopwords |= {\"nya\",\"jurusan\",\"jurus\",\"the\",\"of\"}\n",
    "\n",
    "from json import loads\n",
    "from ast import literal_eval\n",
    "from pickle import load\n",
    "from re import sub\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datetime import date,timedelta\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = pre.remove_tag(text) #Remove Tag\n",
    "    text = pre.lower(text) #Lower\n",
    "    text = pre.remove_link(text) #Remove Link\n",
    "    text = pre.alphabet_only(text) #Get Alphabet\n",
    "    text = sub(r'sobat pintar','',text) # sorry:(\n",
    "    text = pre.remove_whitespace(text) #Remove Whitespace\n",
    "    text = [token.text for token in nlp(text)] #Token\n",
    "    text = pre.slang(text)\n",
    "    text = [token.lemma_ for token in nlp(text) if token.lemma_ not in stopwords] #Lemma & stopword\n",
    "    \n",
    "    return text\n",
    "\n",
    "def load_model():\n",
    "    #Load Model\n",
    "    loaded_model = LdaModel.load('../data/lda.h5')\n",
    "    \n",
    "    #Load Corpus\n",
    "    file = open('../data/corpus_LDA.pkl','rb')\n",
    "    loaded_corpus = load(file)\n",
    "    \n",
    "    #Load Dictionary\n",
    "    file = open('../data/dictionary_LDA.gensim','rb')\n",
    "    loaded_dict = load(file)\n",
    "    \n",
    "    #Load TFIDF\n",
    "    file = open('../data/tfidf.h5','rb')\n",
    "    loaded_tfidf = load(file)\n",
    "    \n",
    "    #Load encoder\n",
    "    dict_encoder = loads(open(\"../data/dict_encoder.txt\", 'r').read())\n",
    "    \n",
    "    return loaded_model,loaded_corpus,loaded_dict,loaded_tfidf,dict_encoder\n",
    "\n",
    "def encode(docId,dict_encoder):\n",
    "    entryId = []\n",
    "    \n",
    "    for i in docId:\n",
    "        result = dict_encoder.get(str(i))\n",
    "        entryId.append(result)\n",
    "        \n",
    "    return entryId    \n",
    "\n",
    "def new_user(event):\n",
    "    entryId = event['entryId']\n",
    "    try:\n",
    "        loaded_model,loaded_corpus,loaded_dict,loaded_tfidf,dict_encoder = load_model()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "#         pass\n",
    "    try:\n",
    "        #Get Doc\n",
    "        statement = \" WHERE entryId = {}\"\n",
    "        data = open_table(['entryId','content'],'BlogsEntry',statement=statement.format(entryId))\n",
    "        text = data[1].values[0]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "#         pass\n",
    "    try:\n",
    "        #Test new document\n",
    "        text = preprocessing(text) #Preprocessing\n",
    "        # Add bigrams and trigrams to docs,minimum count 10 means only that appear 10 times or more.\n",
    "        bigram = Phrases([text], min_count=3)\n",
    "        for token in bigram[text]:\n",
    "            if '_' in token:\n",
    "                # Token is a bigram, add to document.\n",
    "                text.append(token)\n",
    "\n",
    "        bow = loaded_dict.doc2bow(text)\n",
    "\n",
    "        vector = loaded_tfidf[bow]  # apply model to the first corpus document\n",
    "        vector = sorted(vector, key=lambda tup: tup[1],reverse=True) #Sort\n",
    "        keyword = [loaded_dict[x[0]] for x in vector]\n",
    "\n",
    "        new_bow = loaded_dict.doc2bow(keyword)\n",
    "    \n",
    "        lda_index = similarities.MatrixSimilarity(loaded_model[loaded_corpus])\n",
    "    \n",
    "        query = lda_index[loaded_model[new_bow]]\n",
    "        # # Sort the similarities\n",
    "        sort_sim = sorted(enumerate(query), key=lambda item: -item[1])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "#         pass\n",
    "    \n",
    "    try:\n",
    "        result = [x[0] for x in sort_sim] #Get Univ ID\n",
    "        result = encode(result,dict_encoder)\n",
    "        result.remove(entryId) #Remove Input EntryId\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "#         pass\n",
    "    \n",
    "    return result[:10]\n",
    "\n",
    "def save_recommendation(entryId,recommendation,last_update):\n",
    "#     last_update = date.today()\n",
    "    \n",
    "    table = \"related_news_lda\"\n",
    "    column = ['entryId','recommendation','updateDate']\n",
    "    value = [entryId,str(recommendation),last_update]\n",
    "    to_db(table,column,value)\n",
    "\n",
    "def get_similar(event):\n",
    "    entryId = event['entryId']\n",
    "\n",
    "    today = date.today()\n",
    "    refreshtime = today - timedelta(days=4)\n",
    "    statement = ' where entryId = {}'\n",
    "    recommendation = open_table_ds(['*'],'related_news_lda',statement=statement.format(entryId))\n",
    "\n",
    "    #First Time\n",
    "    if not recommendation:\n",
    "        result = new_user(event)\n",
    "        save_recommendation(entryId,result,today)\n",
    "\n",
    "    else:\n",
    "        recommendation = recommendation[0]\n",
    "        recommendation_refreshtime = recommendation[2]\n",
    "        #Refresh Time\n",
    "        if refreshtime > recommendation_refreshtime:\n",
    "            result = get_similar(event)\n",
    "            \n",
    "            statement = ' where entryId = {}'\n",
    "            data = {\n",
    "                'recommedation':result,\n",
    "                'updateDate':today\n",
    "            }\n",
    "            update_db('related_news_lda',data,statement=statement.format(entryId))\n",
    "#             replace_to_database_news(userId,str(result['recommendation']),today)\n",
    "        #Already Exist\n",
    "        else:\n",
    "            result = literal_eval(recommendation[1])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import process_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39314316, 24815998, 48654656, 53949884, 8883845]\n",
      "0.015625\n"
     ]
    }
   ],
   "source": [
    "t = process_time()\n",
    "#do some stuff\n",
    "event = {'entryId':63282109}\n",
    "print(get_similar(event))\n",
    "elapsed_time = process_time() - t\n",
    "print(elapsed_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
