{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336.109375\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'module')))\n",
    "from openTable import *\n",
    "from filepath import *\n",
    "\n",
    "from re import sub\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import gensim\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import Phrases\n",
    "from gensim import corpora\n",
    "\n",
    "# from ast import literal_eval\n",
    "from pickle import dump\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import pandas as pd\n",
    "from json import loads,dumps\n",
    "\n",
    "# import spacy\n",
    "from spacy.lang.id import Indonesian,stop_words\n",
    "nlp = Indonesian()  # use directly\n",
    "stopwords = stop_words.STOP_WORDS \n",
    "stopwords |= {\"nya\",\"jurusan\",\"jurus\",\"the\",\"of\"}\n",
    "\n",
    "# #Akronim\n",
    "def slang(tokenized_sentence):\n",
    "    slang_word_dict = loads(open(\"../data/slang_word_dict.txt\", 'r').read())\n",
    "\n",
    "    for index in range(len(tokenized_sentence)):\n",
    "        for key, value in slang_word_dict.items():\n",
    "            for v in value:\n",
    "                if tokenized_sentence[index] == v:\n",
    "                    tokenized_sentence[index] = key\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "    return \" \".join(tokenized_sentence)\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = sub('<[^<]+?>', '', str(text)) #remove tag\n",
    "    text = text.lower() #lower\\n\",\n",
    "    text = sub(r'[^a-z]',' ',str(text)) #get alphabet only\n",
    "    text = sub(r'\\s+', ' ', text) #remove white space\n",
    "    text = sub(r'sobat pintar','',text) # sorry:(\n",
    "    text = [token.text for token in nlp(text)] #Token\n",
    "    text = slang(text)#slang word\n",
    "    text = sub(r'\\s+', ' ', text) #remove white space\n",
    "    text = [token.lemma_ for token in nlp(text) if token.lemma_ not in stopwords] #Lemma & stopword\n",
    "    \n",
    "    return text\n",
    "\n",
    "def get_data():\n",
    "    data = open_table(['entryId','content'],'BlogsEntry')\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_best_topic(dictionary, corpus, texts, limit, start):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    \n",
    "    for num_topics in range(start, limit):\n",
    "        model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=666)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "        \n",
    "    #get best model\n",
    "    max_value = max(coherence_values)\n",
    "    max_index = coherence_values.index(max_value)\n",
    "    best_model = model_list[max_index]\n",
    "        \n",
    "    return best_model\n",
    "\n",
    "def make_corpus(data):\n",
    "    #Make list of list\n",
    "    mylist = []\n",
    "    for i,j in data.iterrows():\n",
    "    #     print(j.content)\n",
    "    #     tmp = literal_eval(j.content)\n",
    "        mylist.append(j.content)\n",
    "\n",
    "    # Add bigrams and trigrams to docs,minimum count 10 means only that appear 10 times or more.\n",
    "    bigram = Phrases(mylist, min_count=10)\n",
    "    for idx in range(len(mylist)):\n",
    "        for token in bigram[mylist[idx]]:\n",
    "            if '_' in token:\n",
    "                # Token is a bigram, add to document.\n",
    "                mylist[idx].append(token)\n",
    "\n",
    "    # Create Dictionary\n",
    "    dictionary = corpora.Dictionary(mylist)\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus = [dictionary.doc2bow(text) for text in mylist]\n",
    "    \n",
    "    dump(corpus, open('../data/corpus_LDA.pkl', 'wb'))\n",
    "    dictionary.save('../data/dictionary_LDA.gensim')\n",
    "    \n",
    "    return mylist,dictionary,corpus\n",
    "    \n",
    "def save_model(model):\n",
    "    #Save Model\n",
    "    model.save('../data/lda.h5')\n",
    "    \n",
    "def train():\n",
    "    dict_encoder = {}\n",
    "\n",
    "    #get data\n",
    "    data = get_data()\n",
    "    data = rename_column(data,{0:'entryId', 1:'content'})\n",
    "    data.content = data.content.apply(preprocessing)\n",
    "\n",
    "    #make corpus\n",
    "    mylist,dictionary,corpus = make_corpus(data)\n",
    "\n",
    "    #make dict encode\n",
    "    for i,j in zip(range(len(mylist)),data.entryId.tolist()):\n",
    "        dict_encoder[i] = j\n",
    "\n",
    "    with open('../data/dict_encoder.txt', 'w') as file:\n",
    "         file.write(dumps(dict_encoder)) # use `json.loads` to do the reverse\n",
    "\n",
    "    start=3\n",
    "    limit=51\n",
    "    best_model = get_best_topic(dictionary, corpus=corpus, texts=mylist, start=start, limit=limit)\n",
    "\n",
    "    save_model(best_model)\n",
    "    \n",
    "train()\n",
    "\n",
    "# t = process_time()\n",
    "# #do some stuff\n",
    "# train()\n",
    "# elapsed_time = process_time() - t\n",
    "# print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from time import process_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = process_time()\n",
    "# #do some stuff\n",
    "# train()\n",
    "# elapsed_time = process_time() - t\n",
    "# print(elapsed_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
