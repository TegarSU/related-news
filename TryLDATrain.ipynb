{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'module')))\n",
    "from openTable import *\n",
    "from filepath import *\n",
    "from preprocessing import preprocessing_text as pre\n",
    "\n",
    "# import gensim\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import Phrases,TfidfModel\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "from pickle import dump\n",
    "from json import dumps\n",
    "from datetime import datetime \n",
    "from re import sub\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import spacy\n",
    "from spacy.lang.id import Indonesian,stop_words\n",
    "nlp = Indonesian()  # use directly\n",
    "stopwords = stop_words.STOP_WORDS \n",
    "stopwords |= {\"nya\",\"jurusan\",\"jurus\",\"the\",\"of\"}\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = pre.remove_tag(text) #Remove Tag\n",
    "    text = pre.remove_whitespace(text) #Remove Whitespace\n",
    "    text = pre.lower(text) #Lower\n",
    "    text = pre.remove_link(text) #Remove Link\n",
    "    text = pre.alphabet_only(text) #Get Alphabet\n",
    "    text = sub(r'sobat pintar','',text) # sorry:(\n",
    "    text = pre.remove_whitespace(text) #Remove Whitespace\n",
    "    text = [token.text for token in nlp(text)] #Token\n",
    "    text = pre.slang(text)\n",
    "    text = [token.lemma_ for token in nlp(text) if token.lemma_ not in stopwords] #Lemma & stopword\n",
    "    \n",
    "    return text\n",
    "\n",
    "def get_data():\n",
    "    data = open_table(['entryId','content'],'BlogsEntry')\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_best_topic(dictionary, corpus, texts, limit, start):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    \n",
    "    for num_topics in range(start, limit):\n",
    "        model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=666)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "        \n",
    "    #get best model\n",
    "    max_value = max(coherence_values)\n",
    "    max_index = coherence_values.index(max_value)\n",
    "    best_model = model_list[max_index]\n",
    "        \n",
    "    return best_model\n",
    "\n",
    "def tfidf_keyword(corpus,dictionary,mylist):\n",
    "    model = TfidfModel(corpus=corpus,id2word=dictionary)  # fit model\n",
    "    model.save('../data/tfidf.h5')\n",
    "    \n",
    "    bow = [dictionary.doc2bow(mylist[i]) for i in range(len(mylist))]\n",
    "    vector = [model[bow[i]] for i in range(len(mylist))]\n",
    "    vector = [sorted(vector[i], key=lambda tup: tup[1],reverse=True) for i in range(len(vector))] #Sort\n",
    "    \n",
    "    feature = []\n",
    "    result = []\n",
    "    \n",
    "    for i in vector:\n",
    "        for j in range(len(i)):\n",
    "            feature.append(dictionary[i[j][0]])\n",
    "\n",
    "        result.append(feature[:150])\n",
    "        feature = []\n",
    "        \n",
    "    return result\n",
    "\n",
    "def make_corpus(data):\n",
    "    #Make list of list\n",
    "    mylist = []\n",
    "    for i,j in data.iterrows():\n",
    "        mylist.append(j.content)\n",
    "\n",
    "    # Add bigrams and trigrams to docs,minimum count 10 means only that appear 10 times or more.\n",
    "    bigram = Phrases(mylist, min_count=8)\n",
    "    for idx in range(len(mylist)):\n",
    "        for token in bigram[mylist[idx]]:\n",
    "            if '_' in token:\n",
    "                # Token is a bigram, add to document.\n",
    "                mylist[idx].append(token)\n",
    "\n",
    "    # Create Dictionary\n",
    "    dictionary = corpora.Dictionary(mylist)\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus = [dictionary.doc2bow(text) for text in mylist]\n",
    "    \n",
    "    dump(corpus, open('../data/corpus_LDA.pkl', 'wb'))\n",
    "    dictionary.save('../data/dictionary_LDA.gensim')\n",
    "    \n",
    "    return mylist,dictionary,corpus\n",
    "    \n",
    "def save_model(lda_model):\n",
    "    #Save Model LDA\n",
    "    lda_model.save('../data/lda.h5')\n",
    "    \n",
    "def train():\n",
    "    dict_encoder = {}\n",
    "    \n",
    "    try:\n",
    "        #Get today Date\n",
    "        date_end = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "        #get data\n",
    "        data = get_data()\n",
    "        data = rename_column(data,{0:'entryId', 1:'content'})\n",
    "        data.content = data.content.apply(preprocessing)\n",
    "        \n",
    "        #Save to Txt\n",
    "        with open('../data/train_news.txt', 'a+') as output:\n",
    "            output.write(\"Get Today Data Success, {} \\n\".format(date_end))\n",
    "        print(\"Get today Date Success\")\n",
    "    except Exception as e:\n",
    "        print(\"Get today Date Failed\",e)\n",
    "        with open('../data/train_news.txt', 'a+') as output:\n",
    "            output.write(\"Get Today Data Failed, {} \\n\".format(date_end))\n",
    "    \n",
    "    try:\n",
    "        #make corpus\n",
    "        mylist,dictionary,corpus = make_corpus(data)\n",
    "        \n",
    "        #make dict encode\n",
    "        for i,j in zip(range(len(mylist)),data.entryId.tolist()):\n",
    "            dict_encoder[i] = j\n",
    "            \n",
    "        with open('../data/dict_encoder.txt', 'w') as file:\n",
    "            file.write(dumps(dict_encoder)) # use `json.loads` to do the reverse\n",
    "        print(\"Make Dictionary and Corpus Success\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Make Dictionary and Corpus Failed\")\n",
    "        \n",
    "    try:\n",
    "        #TF-IDF\n",
    "        mylist = tfidf_keyword(corpus,dictionary,mylist)\n",
    "        print(\"get keyword Success\")\n",
    "    except Exception as e:\n",
    "        print(\"get keyword Failed\",e)\n",
    "        \n",
    "    try:\n",
    "        start=3\n",
    "        limit=51\n",
    "        best_model = get_best_topic(dictionary, corpus=corpus, texts=mylist, start=start, limit=limit)\n",
    "        \n",
    "        save_model(best_model)\n",
    "        print(\"Train LDA Success\")\n",
    "    except Exception as e:\n",
    "        print(\"Train LDA Failed\",e)\n",
    "    \n",
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import process_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get today Date Success\n",
      "Make Dictionary and Corpus Success\n",
      "get keyword Success\n",
      "Train LDA Success\n",
      "363.515625\n"
     ]
    }
   ],
   "source": [
    "t = process_time()\n",
    "#do some stuff\n",
    "train()\n",
    "elapsed_time = process_time() - t\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
