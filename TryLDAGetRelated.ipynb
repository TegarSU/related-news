{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'module')))\n",
    "from openTable import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import gensim\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# import spacy\n",
    "from spacy.lang.id import Indonesian\n",
    "nlp = Indonesian()  # use directly\n",
    "stopwords = spacy.lang.id.stop_words.STOP_WORDS \n",
    "stopwords |= {\"nya\",\"jurusan\",\"jurus\",\"the\",\"of\"}\n",
    "\n",
    "from json import loads\n",
    "from ast import literal_eval\n",
    "from pickle import load\n",
    "\n",
    "from gensim import similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Akronim\n",
    "def slang(tokenized_sentence):\n",
    "    slang_word_dict = loads(open(\"slang_word_dict.txt\", 'r').read())\n",
    "\n",
    "    for index in range(len(tokenized_sentence)):\n",
    "        for key, value in slang_word_dict.items():\n",
    "            for v in value:\n",
    "                if tokenized_sentence[index] == v:\n",
    "                    tokenized_sentence[index] = key\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "    return \" \".join(tokenized_sentence)\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = re.sub('<[^<]+?>', '', str(text)) #remove tag\n",
    "    text = text.lower() #lower\\n\",\n",
    "    text = re.sub(r'[^a-z]',' ',str(text)) #get alphabet only\n",
    "    text = re.sub(r'\\s+', ' ', text) #remove white space\n",
    "    text = re.sub(r'sobat pintar','',text) # sorry:(\n",
    "    text = [token.text for token in nlp(text)] #Token\n",
    "    text = slang(text)#slang word\n",
    "    text = re.sub(r'\\s+', ' ', text) #remove white space\n",
    "    text = [token.lemma_ for token in nlp(text) if token.lemma_ not in stopwords] #Lemma & stopword\n",
    "    \n",
    "    return text\n",
    "\n",
    "def load_model():\n",
    "    #Load Model\n",
    "    loaded_model = LdaModel.load('lda.h5')\n",
    "    \n",
    "    #Load Corpus\n",
    "    file = open('corpus_LDA.pkl','rb')\n",
    "    loaded_corpus = load(file)\n",
    "    \n",
    "    #Load Dictionary\n",
    "    file = open('dictionary_LDA.gensim','rb')\n",
    "    loaded_dict = load(file)\n",
    "    \n",
    "    #Load encoder\n",
    "    dict_encoder = loads(open(\"../data/dict_encoder.txt\", 'r').read())\n",
    "    \n",
    "    return loaded_model,loaded_corpus,loaded_dict,dict_encoder\n",
    "\n",
    "def encode(docId,dict_encoder):\n",
    "    entryId = []\n",
    "    \n",
    "    for i in docId:\n",
    "        result = dict_encoder.get(str(i))\n",
    "        entryId.append(result)\n",
    "        \n",
    "    return entryId    \n",
    "\n",
    "def get_similar(entryId):\n",
    "    loaded_model,loaded_corpus,loaded_dict,dict_encoder = load_model()\n",
    "    \n",
    "    #Get Doc\n",
    "    statement = \" WHERE entryId = {}\"\n",
    "    data = open_table(['entryId','content'],'BlogsEntry',statement=statement.format(entryId))\n",
    "    text = data[1].values[0]\n",
    "    \n",
    "    #Test new document\n",
    "    bow = loaded_dict.doc2bow(preprocessing(text))\n",
    "    \n",
    "    lda_index = similarities.MatrixSimilarity(loaded_model[loaded_corpus])\n",
    "    \n",
    "    query = lda_index[loaded_model[bow]]\n",
    "    # # Sort the similarities\n",
    "    sort_sim = sorted(enumerate(query), key=lambda item: -item[1])\n",
    "    \n",
    "    result = [x[0] for x in sort_sim] #Get Univ ID\n",
    "    \n",
    "    result = encode(result[:5],dict_encoder)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import process_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63282109, 65703837, 45171760, 11024887, 63456384]\n",
      "4.5625\n"
     ]
    }
   ],
   "source": [
    "t = process_time()\n",
    "#do some stuff\n",
    "print(get_similar(63282109))\n",
    "elapsed_time = process_time() - t\n",
    "print(elapsed_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
